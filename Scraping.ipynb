{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmcbxiPCrk-6"
   },
   "source": [
    "# Scraping Depression Related Tweets from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scraping tweets using snscrape, it is a scraper for social networking services (SNS). It scrapes things like user profiles,\n",
    "hashtags, or searches and returns the discovered items.'''\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caFDs7VLr_Xj"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdx4GWco8qyG"
   },
   "source": [
    "### Tag: #sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4053922,
     "status": "ok",
     "timestamp": 1626787503326,
     "user": {
      "displayName": "Devesh Mishra",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj6ibC0VE6NLTN5OUlRXalSY6NbAhldHllKnSlMEw=s64",
      "userId": "03033184790263734268"
     },
     "user_tz": -330
    },
    "id": "wHL1MBFgkz79",
    "outputId": "3a0c3acd-8f5d-4f41-cf6d-52a4c0e58fde",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "dep_sadness = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#sadness since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    dep_sadness.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang, tweet.url, \n",
    "                        tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "dep_sadness = pd.DataFrame(dep_sadness, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text','Language', 'URL','Mention', 'Hashtags'])\n",
    "dep_sadness.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_sadness.to_csv(\"dep_sadness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaJvDvPu8wxZ"
   },
   "source": [
    "### Tag: #depressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wnmtj57j2Tka"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "dep_depressed = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#depressed since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    dep_depressed.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang, tweet.url, \n",
    "                        tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "dep_depressed = pd.DataFrame(dep_depressed, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "dep_depressed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_depressed.to_csv(\"dep_depressed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzz5myp080kW"
   },
   "source": [
    "### Tag: #loneliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0ONx0wq52djX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "dep_loneliness = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#loneliness since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    dep_loneliness.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                  tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "dep_loneliness = pd.DataFrame(dep_loneliness, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "dep_loneliness.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_loneliness.to_csv(\"dep_loneliness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyIS3HzL83Wm"
   },
   "source": [
    "### Tag: #depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rajlFro12dZB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "dep_depression = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#depression since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    dep_depression.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                  tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "dep_depression = pd.DataFrame(dep_depression, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "dep_depression.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r2k5gtG0-QUt"
   },
   "outputs": [],
   "source": [
    "dep_depression.to_csv(\"dep_depression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C3aEoKC6hLD"
   },
   "source": [
    "# Scraping Non-Depression Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHtBi2In9i4c"
   },
   "source": [
    "### Tag: #happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9zCiP65W6kDi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets for the these tags.\n",
    "non_depress_tags = [\"#happy\", \"#selflove\", \"#positivevibes\", \"#inspiration\"]\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "non_dep_happy = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#happy since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    non_dep_happy.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                  tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "non_dep_happy = pd.DataFrame(non_dep_happy, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "non_dep_happy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5iY9ud7E_HpX"
   },
   "outputs": [],
   "source": [
    "non_dep_happy.to_csv(\"non_dep_happy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcdFkUk19l1S"
   },
   "source": [
    "### Tag: #selflove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JxVyIyTk7fCt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "non_dep_selflove = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#selflove since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    non_dep_selflove.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                  tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "non_dep_selflove = pd.DataFrame(non_dep_selflove, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "non_dep_selflove.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Wemg9DVE_PTg"
   },
   "outputs": [],
   "source": [
    "non_dep_selflove.to_csv(\"non_dep_selflove.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jsCmzTS9oB9"
   },
   "source": [
    "### Tag: #positivevibes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gFmIm-qS7e9o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "non_dep_positivevibes = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#positivevibes since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    non_dep_positivevibes.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                  tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "non_dep_positivevibes = pd.DataFrame(non_dep_positivevibes, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "non_dep_positivevibes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SDkSTp6Q_V_q"
   },
   "outputs": [],
   "source": [
    "non_dep_positivevibes.to_csv(\"non_dep_positivevibes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMH28lJ59qu-"
   },
   "source": [
    "### Tag: #inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HueFTeJN7e3I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5501, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating list to append tweet data to\n",
    "non_dep_inspiration = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('#inspiration since:2019-01-01 until:2020-12-31').get_items()):\n",
    "    if i>5500:\n",
    "        break\n",
    "    non_dep_inspiration.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.lang,\n",
    "                                tweet.url, tweet.mentionedUsers, tweet.hashtags])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above\n",
    "non_dep_inspiration = pd.DataFrame(non_dep_inspiration, columns=['Datetime', 'Tweet Id','Username',\n",
    "                                                     'Text', 'Language', 'URL','Mention', 'Hashtags'])\n",
    "non_dep_inspiration.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving it in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mSk2VYA2_dxL"
   },
   "outputs": [],
   "source": [
    "non_dep_inspiration.to_csv(\"non_dep_inspiration.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZ87LVcfNHrmDrOZmmtUq+",
   "collapsed_sections": [],
   "mount_file_id": "1yMME2P-wsi7rjvvr1Hb8MpyhHIhGjCi9",
   "name": "Data Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
