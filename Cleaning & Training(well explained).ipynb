{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYQrvmMJ6EI4"
   },
   "source": [
    "# Cleaning Depression Related Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAlgbP1U3iS1"
   },
   "source": [
    "### Importing Extracted Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fber8zhx3enx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SGniHz1zpnYV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (22004, 9)\n",
      "Unique tweets: 21621\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"dep_sadness.csv\")\n",
    "df_2 = pd.read_csv(\"dep_depressed.csv\")\n",
    "df_3 = pd.read_csv(\"dep_loneliness.csv\")\n",
    "df_4 = pd.read_csv(\"dep_depression.csv\")\n",
    "\n",
    "df = pd.concat([df_1, df_2, df_3, df_4], ignore_index=True, axis='rows')\n",
    "\n",
    "print(\"Shape:\",df.shape)\n",
    "print(\"Unique tweets:\", len(df['Tweet Id'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping/removing duplicate tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aEHisS5QqG7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (21621, 9)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset =[\"Tweet Id\"])\n",
    "print(\"Shape:\",df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_0XzICRBqqWF",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>URL</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-30 23:05:25+00:00</td>\n",
       "      <td>1344419347524165638</td>\n",
       "      <td>seesawlesbian</td>\n",
       "      <td>‚Äúbts is a trend‚Äù #bts #btsarmy #btsbts #hashta...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://twitter.com/seesawlesbian/status/13444...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['bts', 'btsarmy', 'btsbts', 'hashtag', 'kpop'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-30 22:51:23+00:00</td>\n",
       "      <td>1344415815249113090</td>\n",
       "      <td>That_Guy_Crash</td>\n",
       "      <td>What it feels like to lose in Mario Kart #mari...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://twitter.com/That_Guy_Crash/status/1344...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['mariokart', 'mariokart8', 'loser', 'lost', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-12-30 22:48:18+00:00</td>\n",
       "      <td>1344415040653422593</td>\n",
       "      <td>MIDAGEDRUNNER</td>\n",
       "      <td>@Lunker58Steele #sadness</td>\n",
       "      <td>und</td>\n",
       "      <td>https://twitter.com/MIDAGEDRUNNER/status/13444...</td>\n",
       "      <td>[User(username='Lunker58Steele', id=707050254,...</td>\n",
       "      <td>['sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-12-30 22:39:13+00:00</td>\n",
       "      <td>1344412753314902022</td>\n",
       "      <td>MBCharacter</td>\n",
       "      <td>https://t.co/X0T941sg7a\\n#help #covid_19 #isol...</td>\n",
       "      <td>und</td>\n",
       "      <td>https://twitter.com/MBCharacter/status/1344412...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['help', 'covid_19', 'isolation', 'sadness', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-12-30 21:58:28+00:00</td>\n",
       "      <td>1344402501307564032</td>\n",
       "      <td>rebeccajchaney</td>\n",
       "      <td>Like everyone, I have so many memories from th...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://twitter.com/rebeccajchaney/status/1344...</td>\n",
       "      <td>[User(username='rebeccajchaney', id=2842089636...</td>\n",
       "      <td>['memory', 'reflection', 'Reflection2020', 'ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Datetime             Tweet Id        Username  \\\n",
       "0           0  2020-12-30 23:05:25+00:00  1344419347524165638   seesawlesbian   \n",
       "1           1  2020-12-30 22:51:23+00:00  1344415815249113090  That_Guy_Crash   \n",
       "2           2  2020-12-30 22:48:18+00:00  1344415040653422593   MIDAGEDRUNNER   \n",
       "3           3  2020-12-30 22:39:13+00:00  1344412753314902022     MBCharacter   \n",
       "4           4  2020-12-30 21:58:28+00:00  1344402501307564032  rebeccajchaney   \n",
       "\n",
       "                                                Text Language  \\\n",
       "0  ‚Äúbts is a trend‚Äù #bts #btsarmy #btsbts #hashta...       en   \n",
       "1  What it feels like to lose in Mario Kart #mari...       en   \n",
       "2                           @Lunker58Steele #sadness      und   \n",
       "3  https://t.co/X0T941sg7a\\n#help #covid_19 #isol...      und   \n",
       "4  Like everyone, I have so many memories from th...       en   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://twitter.com/seesawlesbian/status/13444...   \n",
       "1  https://twitter.com/That_Guy_Crash/status/1344...   \n",
       "2  https://twitter.com/MIDAGEDRUNNER/status/13444...   \n",
       "3  https://twitter.com/MBCharacter/status/1344412...   \n",
       "4  https://twitter.com/rebeccajchaney/status/1344...   \n",
       "\n",
       "                                             Mention  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  [User(username='Lunker58Steele', id=707050254,...   \n",
       "3                                                NaN   \n",
       "4  [User(username='rebeccajchaney', id=2842089636...   \n",
       "\n",
       "                                            Hashtags  \n",
       "0  ['bts', 'btsarmy', 'btsbts', 'hashtag', 'kpop'...  \n",
       "1  ['mariokart', 'mariokart8', 'loser', 'lost', '...  \n",
       "2                                        ['sadness']  \n",
       "3  ['help', 'covid_19', 'isolation', 'sadness', '...  \n",
       "4  ['memory', 'reflection', 'Reflection2020', 'ca...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper has scraped URLs of the tweets and has not scraped URLs that are in the tweets. We will need to drop the row \"URL\" and will have to find a way to find URLs from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (21621, 7)\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0', 'URL'], axis='columns', inplace=True)\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QleNjxBssHKM"
   },
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping tweets that are not in English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (17112, 7)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Language'].str.contains('en')]\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSRiEFR6sS9a"
   },
   "source": [
    "### Remove entries containing URLs, they may be promotional tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qraCSg-irFy-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (6333, 7)\n"
     ]
    }
   ],
   "source": [
    "s = ['https:', 'Https:']\n",
    "df = df[~df.Text.str.contains('|'.join(s))]\n",
    "\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlgPkcW3xa2x"
   },
   "source": [
    "### Remove entries with at (i.e. @) mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "87QtX_-KtQE3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4029, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[pd.isnull(df['Mention'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-12-30 17:30:10+00:00</td>\n",
       "      <td>1344334980252655616</td>\n",
       "      <td>nobleregulus</td>\n",
       "      <td>not a single comment on my recent fic yet.. üòñüò£...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-12-30 15:28:41+00:00</td>\n",
       "      <td>1344304406536269825</td>\n",
       "      <td>mariasophiemegn</td>\n",
       "      <td>its so sad when u take down all the christmas ...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sad', 'sadness', 'Christmas', 'christmasisov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-12-30 14:05:12+00:00</td>\n",
       "      <td>1344283398177779714</td>\n",
       "      <td>ClosetsWidows</td>\n",
       "      <td>If love could have saved you, I know you would...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['grief', 'sadness', 'widowed', 'widows', 'dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-12-30 12:42:32+00:00</td>\n",
       "      <td>1344262594694172672</td>\n",
       "      <td>BookSnip</td>\n",
       "      <td>too much happiness always overflowed into tear...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['happiness', 'sadness', 'crying']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-12-30 06:55:25+00:00</td>\n",
       "      <td>1344175239358341120</td>\n",
       "      <td>Isla_Plastic18</td>\n",
       "      <td>Sometimes it takes a little bit of sadness to ...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Islabot', 'Sadness', 'Happiness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-12-30 05:49:04+00:00</td>\n",
       "      <td>1344158544484941824</td>\n",
       "      <td>linzmwilliams</td>\n",
       "      <td>Sad that friendships are fractured because of ...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['friendship', 'sadness', 'COVID19', 'scared']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-12-30 04:55:55+00:00</td>\n",
       "      <td>1344145166165131266</td>\n",
       "      <td>ariadnedreams</td>\n",
       "      <td>I have an IV drip of sorrow \\nI keep thinking ...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['poem', 'sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2020-12-30 04:40:52+00:00</td>\n",
       "      <td>1344141380235489287</td>\n",
       "      <td>KlevesAnna</td>\n",
       "      <td>Good night.....Good bye.....both start with go...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness', 'healing', 'broken']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2020-12-30 03:10:41+00:00</td>\n",
       "      <td>1344118682415366144</td>\n",
       "      <td>letra_at_musika</td>\n",
       "      <td>One day before New Year's Eve, everything turn...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['letraatmusika', 'music', 'poetry', 'musika',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2020-12-30 02:51:00+00:00</td>\n",
       "      <td>1344113732792807424</td>\n",
       "      <td>CORTherapist</td>\n",
       "      <td>Healthy expressions of #sadness move the sadne...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2020-12-30 01:00:00+00:00</td>\n",
       "      <td>1344085794760380416</td>\n",
       "      <td>butterfli323</td>\n",
       "      <td>My sweet doggo Yoda passed away today and I am...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness', 'dogsarelove', 'hurting']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2020-12-29 21:59:25+00:00</td>\n",
       "      <td>1344040352387371009</td>\n",
       "      <td>DoonMothafucka</td>\n",
       "      <td>why do toilet paper companies even bother putt...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness', 'agony', 'depression']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2020-12-29 20:43:09+00:00</td>\n",
       "      <td>1344021159394889735</td>\n",
       "      <td>rzbckgrl</td>\n",
       "      <td>Omg.  Of COURSE the game is cancelled.  2020!!...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2020-12-29 18:55:49+00:00</td>\n",
       "      <td>1343994145896091652</td>\n",
       "      <td>sadhaiku4u</td>\n",
       "      <td>a night of artists\\nwhere does my soul feel at...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['sadness']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020-12-29 12:59:11+00:00</td>\n",
       "      <td>1343904399119286274</td>\n",
       "      <td>Michele96691414</td>\n",
       "      <td>Letting go with #love. Hanging on to #bitterne...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['love', 'bitterness', 'Disappointment', 'ange...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Datetime             Tweet Id         Username  \\\n",
       "23  2020-12-30 17:30:10+00:00  1344334980252655616     nobleregulus   \n",
       "28  2020-12-30 15:28:41+00:00  1344304406536269825  mariasophiemegn   \n",
       "30  2020-12-30 14:05:12+00:00  1344283398177779714    ClosetsWidows   \n",
       "33  2020-12-30 12:42:32+00:00  1344262594694172672         BookSnip   \n",
       "44  2020-12-30 06:55:25+00:00  1344175239358341120   Isla_Plastic18   \n",
       "46  2020-12-30 05:49:04+00:00  1344158544484941824    linzmwilliams   \n",
       "49  2020-12-30 04:55:55+00:00  1344145166165131266    ariadnedreams   \n",
       "51  2020-12-30 04:40:52+00:00  1344141380235489287       KlevesAnna   \n",
       "58  2020-12-30 03:10:41+00:00  1344118682415366144  letra_at_musika   \n",
       "60  2020-12-30 02:51:00+00:00  1344113732792807424     CORTherapist   \n",
       "68  2020-12-30 01:00:00+00:00  1344085794760380416     butterfli323   \n",
       "72  2020-12-29 21:59:25+00:00  1344040352387371009   DoonMothafucka   \n",
       "79  2020-12-29 20:43:09+00:00  1344021159394889735         rzbckgrl   \n",
       "82  2020-12-29 18:55:49+00:00  1343994145896091652       sadhaiku4u   \n",
       "97  2020-12-29 12:59:11+00:00  1343904399119286274  Michele96691414   \n",
       "\n",
       "                                                 Text Language Mention  \\\n",
       "23  not a single comment on my recent fic yet.. üòñüò£...       en     NaN   \n",
       "28  its so sad when u take down all the christmas ...       en     NaN   \n",
       "30  If love could have saved you, I know you would...       en     NaN   \n",
       "33  too much happiness always overflowed into tear...       en     NaN   \n",
       "44  Sometimes it takes a little bit of sadness to ...       en     NaN   \n",
       "46  Sad that friendships are fractured because of ...       en     NaN   \n",
       "49  I have an IV drip of sorrow \\nI keep thinking ...       en     NaN   \n",
       "51  Good night.....Good bye.....both start with go...       en     NaN   \n",
       "58  One day before New Year's Eve, everything turn...       en     NaN   \n",
       "60  Healthy expressions of #sadness move the sadne...       en     NaN   \n",
       "68  My sweet doggo Yoda passed away today and I am...       en     NaN   \n",
       "72  why do toilet paper companies even bother putt...       en     NaN   \n",
       "79  Omg.  Of COURSE the game is cancelled.  2020!!...       en     NaN   \n",
       "82  a night of artists\\nwhere does my soul feel at...       en     NaN   \n",
       "97  Letting go with #love. Hanging on to #bitterne...       en     NaN   \n",
       "\n",
       "                                             Hashtags  \n",
       "23                                        ['sadness']  \n",
       "28  ['sad', 'sadness', 'Christmas', 'christmasisov...  \n",
       "30  ['grief', 'sadness', 'widowed', 'widows', 'dea...  \n",
       "33                 ['happiness', 'sadness', 'crying']  \n",
       "44                ['Islabot', 'Sadness', 'Happiness']  \n",
       "46     ['friendship', 'sadness', 'COVID19', 'scared']  \n",
       "49                                ['poem', 'sadness']  \n",
       "51                   ['sadness', 'healing', 'broken']  \n",
       "58  ['letraatmusika', 'music', 'poetry', 'musika',...  \n",
       "60                                        ['sadness']  \n",
       "68              ['sadness', 'dogsarelove', 'hurting']  \n",
       "72                 ['sadness', 'agony', 'depression']  \n",
       "79                                        ['sadness']  \n",
       "82                                        ['sadness']  \n",
       "97  ['love', 'bitterness', 'Disappointment', 'ange...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "542kLO3ZzFUj"
   },
   "source": [
    "## Make tweet column hastag free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FCo8YIVZtP_p",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  Depression\n",
      "23   not a single comment on my recent fic yet.. üòñüò£üòî            1\n",
      "28  its so sad when u take down all the christmas ...           1\n",
      "30  If love could have saved you, I know you would...           1\n",
      "33  too much happiness always overflowed into tear...           1\n",
      "44  Sometimes it takes a little bit of sadness to ...           1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(lambda x: re.sub(r'#\\w+', '', x))\n",
    "df[\"Depression\"] = 1\n",
    "df.drop(['Tweet Id', 'Mention', 'Hashtags', 'Username', 'Datetime', 'Language'],\n",
    "        axis='columns', inplace=True)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcuojlOQB4Ff"
   },
   "source": [
    "# Cleaning Non-Depressive Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZNcS3i5CmbR"
   },
   "source": [
    "### Importing Extracted Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i096A9i_CyF8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (22004, 9)\n",
      "Unique tweets: 20823\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"non_dep_happy.csv\")\n",
    "df_2 = pd.read_csv(\"non_dep_selflove.csv\")\n",
    "df_3 = pd.read_csv(\"non_dep_positivevibes.csv\")\n",
    "df_4 = pd.read_csv(\"non_dep_inspiration.csv\")\n",
    "\n",
    "df_non = pd.concat([df_1, df_2, df_3, df_4],ignore_index=True,axis='rows')\n",
    "\n",
    "print(\"Shape:\", df_non.shape)\n",
    "print(\"Unique tweets:\", len(df_non['Tweet Id'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping/removing duplicate tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "58F_GTXCCyGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20823, 9)\n"
     ]
    }
   ],
   "source": [
    "df_non = df_non.drop_duplicates(subset =[\"Tweet Id\"])\n",
    "print(\"Shape:\",df_non.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6GWDqNqfCyGI",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20823, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>URL</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-30 23:59:58+00:00</td>\n",
       "      <td>1344433078320521217</td>\n",
       "      <td>txtraveltegal</td>\n",
       "      <td>#love  #TFLers #instagood #tweegram #photooftheday #me #instamood #cute #iphonesia #summer #tbt ...</td>\n",
       "      <td>und</td>\n",
       "      <td>https://twitter.com/txtraveltegal/status/1344433078320521217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['love', 'TFLers', 'instagood', 'tweegram', 'photooftheday', 'me', 'instamood', 'cute', 'iphones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-30 23:59:31+00:00</td>\n",
       "      <td>1344432963954438146</td>\n",
       "      <td>pr_deciel</td>\n",
       "      <td>„ÅÇ„Å£„Å®„ÅÑ„ÅÜÈñì„Å´„ÄÅ‰ªäÂπ¥ÊúÄÂæå„ÅÆÊó•ÔºùÂ§ßÊô¶Êó•„Å®„Å™„Çä„Åæ„Åó„Åü\\n\\n„ÇÑ„ÇäÊÆã„Åó„Åü„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÅãÔºü\\n\\n„Éá„Ç∑„Çß„É´„ÅÆ„ÄåÁ¶èË¢ã„ÄçË≤∑„ÅÑ„Åæ„Åó„Åü„ÅãÔºü\\n\\n„Åü„ÅÑ„Å∏„Çì„ÄÅÂ§¢„Å´Âá∫„Å°„ÇÉ„ÅÑ„Åæ„Åô„ÇàÔΩó\\n\\nÂπ¥Êú´Âπ¥Âßã„Å†„Åë„ÅÆÊï∞ÈáèÈôê...</td>\n",
       "      <td>ja</td>\n",
       "      <td>https://twitter.com/pr_deciel/status/1344432963954438146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Á¶èË¢ã', 'HAPPY', 'ÈôêÂÆö', '2021Âπ¥', 'Êñ∞Êò•', 'ÂåñÁ≤ßÂìÅ', 'Â§¢„Å´Âá∫„Çã', '„ÇÑ„ÇäÊÆã„Åó', 'Êï∞ÈáèÈôêÂÆö']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-12-30 23:59:13+00:00</td>\n",
       "      <td>1344432887035256834</td>\n",
       "      <td>braintickler_</td>\n",
       "      <td>JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize. #humor #meme...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://twitter.com/braintickler_/status/1344432887035256834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-12-30 23:59:07+00:00</td>\n",
       "      <td>1344432862381051904</td>\n",
       "      <td>PandaMartini</td>\n",
       "      <td>Dear writing community, I want a #penpal. I have #stickers, a #cricut, and lots of #stationary. ...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://twitter.com/PandaMartini/status/1344432862381051904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['penpal', 'stickers', 'cricut', 'stationary', 'cute', 'gifts', 'notgoingout', 'Lonely', 'spread...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-12-30 23:57:15+00:00</td>\n",
       "      <td>1344432391683715072</td>\n",
       "      <td>Rhaulli_Panda</td>\n",
       "      <td>#merrychristmasüéÑ #merryxmas #happy #newyear #happynewyear #happynewyear2021 #2021 #2021Êò•Â©ö #dicie...</td>\n",
       "      <td>es</td>\n",
       "      <td>https://twitter.com/Rhaulli_Panda/status/1344432391683715072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['merrychristmas', 'merryxmas', 'happy', 'newyear', 'happynewyear', 'happynewyear2021', '2021Êò•Â©ö'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Datetime             Tweet Id       Username  \\\n",
       "0           0  2020-12-30 23:59:58+00:00  1344433078320521217  txtraveltegal   \n",
       "1           1  2020-12-30 23:59:31+00:00  1344432963954438146      pr_deciel   \n",
       "2           2  2020-12-30 23:59:13+00:00  1344432887035256834  braintickler_   \n",
       "3           3  2020-12-30 23:59:07+00:00  1344432862381051904   PandaMartini   \n",
       "4           4  2020-12-30 23:57:15+00:00  1344432391683715072  Rhaulli_Panda   \n",
       "\n",
       "                                                                                                  Text  \\\n",
       "0  #love  #TFLers #instagood #tweegram #photooftheday #me #instamood #cute #iphonesia #summer #tbt ...   \n",
       "1  „ÅÇ„Å£„Å®„ÅÑ„ÅÜÈñì„Å´„ÄÅ‰ªäÂπ¥ÊúÄÂæå„ÅÆÊó•ÔºùÂ§ßÊô¶Êó•„Å®„Å™„Çä„Åæ„Åó„Åü\\n\\n„ÇÑ„ÇäÊÆã„Åó„Åü„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÅãÔºü\\n\\n„Éá„Ç∑„Çß„É´„ÅÆ„ÄåÁ¶èË¢ã„ÄçË≤∑„ÅÑ„Åæ„Åó„Åü„ÅãÔºü\\n\\n„Åü„ÅÑ„Å∏„Çì„ÄÅÂ§¢„Å´Âá∫„Å°„ÇÉ„ÅÑ„Åæ„Åô„ÇàÔΩó\\n\\nÂπ¥Êú´Âπ¥Âßã„Å†„Åë„ÅÆÊï∞ÈáèÈôê...   \n",
       "2  JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize. #humor #meme...   \n",
       "3  Dear writing community, I want a #penpal. I have #stickers, a #cricut, and lots of #stationary. ...   \n",
       "4  #merrychristmasüéÑ #merryxmas #happy #newyear #happynewyear #happynewyear2021 #2021 #2021Êò•Â©ö #dicie...   \n",
       "\n",
       "  Language                                                           URL  \\\n",
       "0      und  https://twitter.com/txtraveltegal/status/1344433078320521217   \n",
       "1       ja      https://twitter.com/pr_deciel/status/1344432963954438146   \n",
       "2       en  https://twitter.com/braintickler_/status/1344432887035256834   \n",
       "3       en   https://twitter.com/PandaMartini/status/1344432862381051904   \n",
       "4       es  https://twitter.com/Rhaulli_Panda/status/1344432391683715072   \n",
       "\n",
       "  Mention  \\\n",
       "0     NaN   \n",
       "1     NaN   \n",
       "2     NaN   \n",
       "3     NaN   \n",
       "4     NaN   \n",
       "\n",
       "                                                                                              Hashtags  \n",
       "0  ['love', 'TFLers', 'instagood', 'tweegram', 'photooftheday', 'me', 'instamood', 'cute', 'iphones...  \n",
       "1                                  ['Á¶èË¢ã', 'HAPPY', 'ÈôêÂÆö', '2021Âπ¥', 'Êñ∞Êò•', 'ÂåñÁ≤ßÂìÅ', 'Â§¢„Å´Âá∫„Çã', '„ÇÑ„ÇäÊÆã„Åó', 'Êï∞ÈáèÈôêÂÆö']  \n",
       "2  ['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...  \n",
       "3  ['penpal', 'stickers', 'cricut', 'stationary', 'cute', 'gifts', 'notgoingout', 'Lonely', 'spread...  \n",
       "4  ['merrychristmas', 'merryxmas', 'happy', 'newyear', 'happynewyear', 'happynewyear2021', '2021Êò•Â©ö'...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Shape:\", df_non.shape)\n",
    "df_non.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (20823, 7)\n"
     ]
    }
   ],
   "source": [
    "df_non.drop(['Unnamed: 0', 'URL'], axis='columns', inplace=True)\n",
    "print(\"Shape:\", df_non.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3vEb4vpCyGI"
   },
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping tweets that are not in English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (15708, 7)\n"
     ]
    }
   ],
   "source": [
    "df_non = df_non[df_non['Language'].str.contains('en')]\n",
    "print(\"Shape:\", df_non.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v33VSUVNCyGJ"
   },
   "source": [
    "### Remove entries containing URLs, they may be promotional tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e0dhewzdCyGK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3835, 7)\n"
     ]
    }
   ],
   "source": [
    "df_non = df_non[~df_non.Text.str.contains('|'.join(s))]\n",
    "print(\"Shape:\", df_non.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPBoyjVwCyGO"
   },
   "source": [
    "### Remove entries with at (i.e. @) mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f5HjU_9MCyGO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3162, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non = df_non.loc[pd.isnull(df_non['Mention'])]\n",
    "df_non.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-30 23:59:13+00:00</td>\n",
       "      <td>1344432887035256834</td>\n",
       "      <td>braintickler_</td>\n",
       "      <td>JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize. #humor #meme...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-30 23:59:07+00:00</td>\n",
       "      <td>1344432862381051904</td>\n",
       "      <td>PandaMartini</td>\n",
       "      <td>Dear writing community, I want a #penpal. I have #stickers, a #cricut, and lots of #stationary. ...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['penpal', 'stickers', 'cricut', 'stationary', 'cute', 'gifts', 'notgoingout', 'Lonely', 'spread...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-12-30 23:42:58+00:00</td>\n",
       "      <td>1344428798624878592</td>\n",
       "      <td>ahmedsamiirr1</td>\n",
       "      <td>#Happy _new _year  ‚ô•‚ô•</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Happy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-12-30 23:41:45+00:00</td>\n",
       "      <td>1344428491647889417</td>\n",
       "      <td>MasugzyoN</td>\n",
       "      <td>Some of us had to make it on our own #Happy New year Eve to all of us</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Happy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-12-30 23:31:06+00:00</td>\n",
       "      <td>1344425812553052174</td>\n",
       "      <td>braintickler_</td>\n",
       "      <td>JOKE OF THE DAY : Have you heard about the film \"Constipation\", you probably haven't because it'...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2020-12-30 23:01:32+00:00</td>\n",
       "      <td>1344418372847628289</td>\n",
       "      <td>braintickler_</td>\n",
       "      <td>JOKE OF THE DAY : What concert costs only 45 cents? 50 cent featuring Nickelback. #humor #memesd...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Datetime             Tweet Id       Username  \\\n",
       "2   2020-12-30 23:59:13+00:00  1344432887035256834  braintickler_   \n",
       "3   2020-12-30 23:59:07+00:00  1344432862381051904   PandaMartini   \n",
       "24  2020-12-30 23:42:58+00:00  1344428798624878592  ahmedsamiirr1   \n",
       "27  2020-12-30 23:41:45+00:00  1344428491647889417      MasugzyoN   \n",
       "37  2020-12-30 23:31:06+00:00  1344425812553052174  braintickler_   \n",
       "73  2020-12-30 23:01:32+00:00  1344418372847628289  braintickler_   \n",
       "\n",
       "                                                                                                   Text  \\\n",
       "2   JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize. #humor #meme...   \n",
       "3   Dear writing community, I want a #penpal. I have #stickers, a #cricut, and lots of #stationary. ...   \n",
       "24                                                                                #Happy _new _year  ‚ô•‚ô•   \n",
       "27                                Some of us had to make it on our own #Happy New year Eve to all of us   \n",
       "37  JOKE OF THE DAY : Have you heard about the film \"Constipation\", you probably haven't because it'...   \n",
       "73  JOKE OF THE DAY : What concert costs only 45 cents? 50 cent featuring Nickelback. #humor #memesd...   \n",
       "\n",
       "   Language Mention  \\\n",
       "2        en     NaN   \n",
       "3        en     NaN   \n",
       "24       en     NaN   \n",
       "27       en     NaN   \n",
       "37       en     NaN   \n",
       "73       en     NaN   \n",
       "\n",
       "                                                                                               Hashtags  \n",
       "2   ['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...  \n",
       "3   ['penpal', 'stickers', 'cricut', 'stationary', 'cute', 'gifts', 'notgoingout', 'Lonely', 'spread...  \n",
       "24                                                                                            ['Happy']  \n",
       "27                                                                                            ['Happy']  \n",
       "37  ['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...  \n",
       "73  ['humor', 'memesdaily', 'rofl', 'jokeoftheday', 'funny', 'lol', 'lmao', 'happy', 'followme', 'lo...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6kCkRU5CyGO"
   },
   "source": [
    "## Make tweet column hastag free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FCo8YIVZtP_p"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear writing community, I want a . I have , a , and lots of . DM me? I also will send   ü•∞ Over 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_new _year  ‚ô•‚ô•</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Some of us had to make it on our own  New year Eve to all of us</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>JOKE OF THE DAY : Have you heard about the film \"Constipation\", you probably haven't because it'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Text  \\\n",
       "2       JOKE OF THE DAY : Whoever invented the knock-knock joke should get a no bell prize.               \n",
       "3   Dear writing community, I want a . I have , a , and lots of . DM me? I also will send   ü•∞ Over 1...   \n",
       "24                                                                                       _new _year  ‚ô•‚ô•   \n",
       "27                                      Some of us had to make it on our own  New year Eve to all of us   \n",
       "37  JOKE OF THE DAY : Have you heard about the film \"Constipation\", you probably haven't because it'...   \n",
       "\n",
       "    Depression  \n",
       "2            0  \n",
       "3            0  \n",
       "24           0  \n",
       "27           0  \n",
       "37           0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non[\"Text\"] = df_non[\"Text\"].apply(lambda x: re.sub(r'#\\w+', '', x))\n",
    "df_non[\"Depression\"] = 0\n",
    "df_non.drop(['Tweet Id', 'Mention', 'Hashtags', 'Username', 'Datetime', 'Language'],\n",
    "           axis='columns', inplace=True)\n",
    "\n",
    "df_non.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining/concatenating dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not a single comment on my recent fic yet.. üòñüò£üòî</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its so sad when u take down all the christmas decorations \\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If love could have saved you, I know you would still be here with me.\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>too much happiness always overflowed into tears of sorrow.\\n\\n-Amy Tan, The Hundred Secret Sense...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sometimes it takes a little bit of sadness to know what happiness is.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  Text  \\\n",
       "0                                                     not a single comment on my recent fic yet.. üòñüò£üòî    \n",
       "1                                      its so sad when u take down all the christmas decorations \\n      \n",
       "2                    If love could have saved you, I know you would still be here with me.\\n             \n",
       "3  too much happiness always overflowed into tears of sorrow.\\n\\n-Amy Tan, The Hundred Secret Sense...   \n",
       "4                             Sometimes it takes a little bit of sadness to know what happiness is.      \n",
       "\n",
       "   Depression  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, df_non], ignore_index=True, axis='rows')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7191, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs:\n",
      " Text          0\n",
      "Depression    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaNs:\\n\", df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Depression/non-Depression related tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4029\n",
       "0    3162\n",
       "Name: Depression, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Depression.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**itertools** is used to iterate over data structures that can be stepped over using a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Field** class models common text processing datatypes that can be represented by tensors. It holds a\n",
    "Vocab object that defines the set of possible values for elements of the field and their corresponding\n",
    "numerical representations. The Field object also holds other parameters relating to how a datatype should be numericalized, such as a tokenization method and the kind of Tensor that should be produced.\n",
    "\n",
    "**BucketIterator** defines an iterator that batches examples of similar lengths together.\n",
    "Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n",
    "See pool for the bucketing procedure used.\n",
    "\n",
    "**TabularDataset** defines a Dataset of columns stored in CSV, TSV, or JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn** is a module and contains different classess that help you build neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.optim** is a package implementing various optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables** are just wrappers for the tensors so you can now easily auto compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Functional** contains some useful functions like activation functions a convolution operations you can use.\n",
    "\n",
    "You would use the torch.nn.Functional conv operations to define a custom layer for example with a convolution operation, but not to define a standard convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pack_padded_sequence:** Packs a Tensor containing padded sequences of variable length.\n",
    "\n",
    "**pad_packed_sequence:** Pads a packed batch of variable length sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But, why do we \"pack\" the sequences?\n",
    "\n",
    "When training RNN (LSTM or GRU or vanilla-RNN), it is difficult to batch the variable length sequences. For example: if the length of sequences in a size 8 batch is [4,6,8,5,4,3,7,8], you will pad all the sequences and that will result in 8 sequences of length 8. You would end up doing 64 computations (8x8), but you needed to do only 45 computations. Moreover, if you wanted to do something fancy like using a bidirectional-RNN, it would be harder to do batch computations just by padding and you might end up doing more computations than required.\n",
    "\n",
    "Instead, PyTorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps and other contains the size of each sequence the batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({'Text': 'tweet', 'Depression': 'target'}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, many users use shortened words (also called contractions) while writing on social media platforms, we need to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_dict = {\"couldn't\": \"could not\", \"ain't\": \"is not\", \"aren't\": \"are not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    \n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_re\n",
    "\n",
    "contractions_re = _get_contractions(contraction_dict)\n",
    "contractions = contraction_dict\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    \n",
    "    return contractions_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "    \n",
    "    # Remove URLs(if left)\n",
    "    text = re.sub(r'<([^>]*)>', ' ', text)\n",
    "    \n",
    "    # Remove emojis(if any)\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    \n",
    "    # remove at mentions(if left)\n",
    "    text = re.sub(r'@\\w+', ' ', text)\n",
    "    \n",
    "    # remove hashtag symbol(if left)\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text)\n",
    "    \n",
    "    # remove numbers(if any)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    text = replace_contractions(text)\n",
    "    pattern = re.compile(r\"[ \\n\\t]+\")\n",
    "    text = pattern.sub(\" \", text)      \n",
    "    text = \"\".join(\"\".join(s)[:2] for _, s in itertools.groupby(text)) \n",
    "    \n",
    "    # Remove all symbols and punctuations, except for some\n",
    "    text = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Since, torchtext faces problem in handling \"\\n\", replacing \"\\n\" with space.\n",
    "df['tweet'] = df.tweet.progress_apply(lambda x: re.sub('\\n', ' ', x))\n",
    "\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s):\n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, include_lengths=True, use_vocab=True)\n",
    "TARGET = Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None, is_target =False)\n",
    "\n",
    "data_fields = [(None, None), (\"tweet\", TEXT), (\"target\", TARGET)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_test(df, test_size=0.2):\n",
    "    \n",
    "    train, val = train_test_split(df, test_size=test_size,random_state=42)\n",
    "    return train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "\n",
    "train_val, test = split_train_test(df, test_size=0.2)\n",
    "train, val = split_train_test(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter pre-trained word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a set of NLP techniques where individual words are mapped to a real-value vector in a high-dimensional space. The vectors are learned in such a way that words that have similar meanings will have similar representation in the vector space.\n",
    "\n",
    "Pre-trained word embeddings are the embeddings learned in one task that are used for solving another similar task. These embeddings are trained on large datasets, saved, and then used for solving other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = torchtext.vocab.Vectors('glove.twitter.27B.100d.txt')\n",
    "TEXT.build_vocab(train_data,max_size = 100_000,vectors=vec)\n",
    "TARGET.build_vocab(train_data)\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(datasets=(train, val, test), batch_sizes=(3,3,3), sort_key=lambda x: len(x.tweet), device=None, sort_within_batch=True, repeat=False)\n",
    "\n",
    "def idxtosent(batch, idx):\n",
    "    return ' '.join([TEXT.vocab.itos[i] for i in batch.tweet[0][:,idx].cpu().data.numpy()])\n",
    "\n",
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, dl, x_field, y_field):\n",
    "        self.dl, self.x_field, self.y_field = dl, x_field, y_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for batch in self.dl:\n",
    "            X = getattr(batch, self.x_field)\n",
    "            y = getattr(batch, self.y_field)\n",
    "            yield (X,y)\n",
    "            \n",
    "train_batch_it = BatchGenerator(train_loader, 'tweet', 'target')\n",
    "vocab_size = len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "n_hidden = 64\n",
    "n_out = 2\n",
    "\n",
    "class ConcatPoolingGRUAdaptive(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, n_hidden, n_out, pretrained_vec, dropout, bidirectional=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.emb.weight.data.copy_(pretrained_vec)\n",
    "        self.emb.weight.requires_grad = False\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.n_hidden, bidirectional=bidirectional)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(self.n_hidden*4, self.n_out)\n",
    "            \n",
    "        else:\n",
    "            self.fc = nn.Linear(2*self.n_hidden, self.n_out)\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def init_hidden(self, batch_size): \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            return torch.zeros((2,batch_size,self.n_hidden))\n",
    "        \n",
    "        else:\n",
    "            return torch.zeros((1,batch_size,self.n_hidden))\n",
    "    \n",
    "    def forward(self, seq, lengths):\n",
    "        \n",
    "        bs = seq.size(1)\n",
    "        self.h = self.init_hidden(bs)\n",
    "        seq = seq.transpose(0,1)\n",
    "        embs = self.emb(seq)\n",
    "        embs = embs.transpose(0,1)\n",
    "        embs = pack_padded_sequence(embs, lengths)\n",
    "        gru_out, self.h = self.gru(embs, self.h)\n",
    "        gru_out, lengths = pad_packed_sequence(gru_out)  \n",
    "        avg_pool = F.adaptive_avg_pool1d(gru_out.permute(1,2,0),1).view(bs,-1)\n",
    "        max_pool = F.adaptive_max_pool1d(gru_out.permute(1,2,0),1).view(bs,-1) \n",
    "        cat = self.dropout(torch.cat([avg_pool,max_pool],dim=1))\n",
    "        outp = self.fc(cat)\n",
    "        \n",
    "        return F.log_softmax(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, num_batch):\n",
    "    \n",
    "    y_true_train = list()\n",
    "    y_pred_train = list()\n",
    "    total_loss_train = 0\n",
    "        \n",
    "    for (X,lengths),y in iterator:\n",
    "            \n",
    "        lengths = lengths.numpy()\n",
    "        opt.zero_grad()\n",
    "        pred = model(X, lengths)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        pred_idx = torch.max(pred, dim=1)[1]\n",
    "        y_true_train += list(y.data.numpy())\n",
    "        y_pred_train += list(pred_idx.data.numpy())\n",
    "        total_loss_train += loss.item()\n",
    "            \n",
    "    train_acc = accuracy_score(y_true_train, y_pred_train)\n",
    "    train_loss = total_loss_train/num_batch\n",
    "        \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, iterator, criterion, num_batch):\n",
    "    \n",
    "    y_true_val = list()\n",
    "    y_pred_val = list()\n",
    "    total_loss_val = 0\n",
    "            \n",
    "    for (X,lengths),y in iterator:\n",
    "                \n",
    "        pred = model(X, lengths.cpu().numpy())\n",
    "        loss = criterion(pred, y)\n",
    "        pred_idx = torch.max(pred, 1)[1]\n",
    "        y_true_val += list(y.cpu().data.numpy())\n",
    "        y_pred_val += list(pred_idx.cpu().data.numpy())\n",
    "        total_loss_val += loss.item()\n",
    "                \n",
    "    valacc = accuracy_score(y_true_val, y_pred_val)\n",
    "    valloss = total_loss_val/num_batch\n",
    "            \n",
    "    return valloss, valacc\n",
    "\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(datasets=(train_data, val_data, test_data), batch_sizes=(32,32,32), sort_key=lambda x: len(x.tweet), device=device, sort_within_batch=True, repeat=False)\n",
    "train_batch_it = BatchGenerator(train_loader, 'tweet', 'target')\n",
    "val_batch_it = BatchGenerator(val_loader, 'tweet', 'target')\n",
    "test_batch_it = BatchGenerator(test_loader, 'tweet', 'target')\n",
    "m = ConcatPoolingGRUAdaptive(vocab_size, embedding_dim, n_hidden, n_out, train_data.fields['tweet'].vocab.vectors, 0.5).to(device)\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, m.parameters()), 1e-3)\n",
    "loss_fn = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Accuracy: 70.03, Train Loss: 0.59\n",
      "Epoch 0: Val Accuracy: 69.37, Val Loss: 0.60\n",
      "Epoch 1: Train Accuracy: 71.62, Train Loss: 0.56\n",
      "Epoch 1: Val Accuracy: 71.23, Val Loss: 0.56\n",
      "Epoch 2: Train Accuracy: 73.17, Train Loss: 0.52\n",
      "Epoch 2: Val Accuracy: 73.02, Val Loss: 0.53\n",
      "Epoch 3: Train Accuracy: 75.67, Train Loss: 0.48\n",
      "Epoch 3: Val Accuracy: 74.37, Val Loss: 0.49\n",
      "Epoch 4: Train Accuracy: 77.03, Train Loss: 0.47\n",
      "Epoch 4: Val Accuracy: 76.34, Val Loss: 0.47\n",
      "Epoch 5: Train Accuracy: 79.32, Train Loss: 0.45\n",
      "Epoch 5: Val Accuracy: 79.26, Val Loss: 0.46\n",
      "Epoch 6: Train Accuracy: 80.54, Train Loss: 0.43\n",
      "Epoch 6: Val Accuracy: 79.75, Val Loss: 0.45\n",
      "Epoch 7: Train Accuracy: 81.53, Train Loss: 0.41\n",
      "Epoch 7: Val Accuracy: 81.23, Val Loss: 0.43\n",
      "Epoch 8: Train Accuracy: 83.01, Train Loss: 0.40\n",
      "Epoch 8: Val Accuracy: 82.37, Val Loss: 0.41\n",
      "Epoch 9: Train Accuracy: 84.11, Train Loss: 0.38\n",
      "Epoch 9: Val Accuracy: 84.02, Val Loss: 0.40\n",
      "Epoch 10: Train Accuracy: 86.03, Train Loss: 0.36\n",
      "Epoch 10: Val Accuracy: 84.37, Val Loss: 0.40\n",
      "Epoch 11: Train Accuracy: 86.34, Train Loss: 0.36\n",
      "Epoch 11: Val Accuracy: 83.08, Val Loss: 0.41\n"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "\n",
    "for epoch in range(epochs):      \n",
    "    \n",
    "    train_loss, train_acc = train(m, iter(train_batch_it), opt, loss_fn, len(train_batch_it))\n",
    "    valid_loss, valid_acc = evaluate(m, iter(val_batch_it), loss_fn, len(val_batch_it))\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        \n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(m.state_dict(), 'tut4-model.pt')\n",
    "          \n",
    "    train_acc = train_acc*100\n",
    "    val_acc = val_acc*100\n",
    "    \n",
    "    print(f'Epoch {epoch}: Train Accuracy: {train_loss:.2f} Train Loss: {train_acc:.2f})\n",
    "    print(f'Epoch {epoch}: Val Accuracy: {valid_loss:.2f} Val Loss: {valid_acc:.2f}')    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGckFKyWcDPEniN5flisU+",
   "collapsed_sections": [
    "IYQrvmMJ6EI4"
   ],
   "name": "Filtering / Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
